# @package _global_
model: idea_test
modal_list:
  - video
  - audio
  - text
projected_dim: 512
dropout: 0.3

batch_size: 32
learning_rate: 0.0001
weight_decay: 0.00000001

i3d_padding_size: 70
video_padding_size: 70
text_padding_size: 70
audio_padding_size: 70

n_frames_one_group: 16

# loss: multilabel_cross_entropy
loss: cross_entropy
# loss: asymmetric_loss
# loss: focal_loss

lr_scheduler:
  mode: max
  factor: 0.5
  patience: 1
  min_lr: 0.0000001

num_classes: 82
video_dim: 768
i3d_dim: 1024
audio_dim: 128
text_dim: 768

early_stopping_patience: 10

co_attention_layer: 1

# pytorch lightning trainer configuration reference:
# https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.trainer.trainer.html#module-pytorch_lightning.trainer.trainer
trainer: 
  default_root_dir: ./log # changed
  gradient_clip_val: 1.0 # changed
  gradient_clip_algorithm: norm
  gpus: 1
  check_val_every_n_epoch: 1
  max_epochs: null
  min_epochs: null
  max_steps: 100000 # changed
  min_steps: null
  limit_train_batches: 1.0
  limit_val_batches: 1.0
  limit_test_batches: 1.0
  limit_predict_batches: 1.0
  val_check_interval: 50 # How often to check the validation set. Use float to check within a training epoch, use int to check every n steps (batches).
  flush_logs_every_n_steps: 100
  log_every_n_steps: 1
  precision: 32
  weights_summary: top
  resume_from_checkpoint: null
  reload_dataloaders_every_epoch: False
  auto_lr_find: True
  auto_scale_batch_size: null # There seems to be some bugs with this, disable it and specify batch size manually
  move_metrics_to_cpu: True

dataset:
  dataset_dir: ./dataset
  text_feature: text_feature_2
  video_feature: video_feature_vit
  audio_feature: audio_feature
  i3d_feature: video_feature