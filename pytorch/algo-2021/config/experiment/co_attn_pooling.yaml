# @package _global_
batch_size: 32
model: co_attn_pooling
attention_head: 8
co_attention_layer: 6
self_attention_layer: 6
pooling_head: 8
pooling_dim: 512
dropout: 0.1

modal_list:
  - video
  - audio
  - text

trainer:
  gpus:
    - 0
  val_check_interval: 50