{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "device = torch.device('cuda:1')\n",
    "\n",
    "# data_type = 'train_5k'\n",
    "# data_type = 'test_5k'\n",
    "data_type = 'test_5k_2nd'\n",
    "\n",
    "data_path = '/home/tione/notebook/algo-2021/dataset/tagging/tagging_dataset_%s' % data_type\n",
    "\n",
    "text_dir = os.path.join(data_path, 'text_txt/tagging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7b094783bb749979ca4d57985eeae88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/412M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "# from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "txt_embedding = BertModel.from_pretrained('bert-base-chinese').to(device)\n",
    "# tokenizer = BertTokenizer.from_pretrained('hfl/chinese-bert-wwm')\n",
    "# txt_embedding = BertModel.from_pretrained('hfl/chinese-bert-wwm').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n"
     ]
    }
   ],
   "source": [
    "# extract text feature\n",
    "\n",
    "for idx, item in enumerate(os.listdir(text_dir)):\n",
    "    if idx % 100 == 0:\n",
    "        print(idx)\n",
    "        \n",
    "    item_name = item.split('.')[0]\n",
    "    \n",
    "#     if os.path.exists(os.path.join('./%s/text_feature_wwm' % data_type, item_name + '.npy')):\n",
    "#         continue\n",
    "    \n",
    "    item_path = os.path.join(text_dir, item)\n",
    "    \n",
    "    with open(item_path, 'r') as file:\n",
    "        text_content = json.load(file)\n",
    "        \n",
    "    asr_list = text_content[\"video_asr\"].split('|')\n",
    "    ocr_list = text_content[\"video_ocr\"].split(\"|\")\n",
    "    \n",
    "    asr_embedding_list = list()\n",
    "    \n",
    "    for text in asr_list:\n",
    "        text_token = tokenizer(text[:512], return_tensors='pt', padding=True)\n",
    "        \n",
    "        for token_item in text_token:\n",
    "            try:\n",
    "                text_token[token_item] = text_token[token_item].to(device)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        text_embedding = txt_embedding(**text_token)['pooler_output']\n",
    "        asr_embedding_list.append(text_embedding.detach().cpu().numpy())\n",
    "    \n",
    "    data = np.concatenate(asr_embedding_list, axis=0)\n",
    "    np.save(os.path.join('./%s/asr_feature' % data_type, item.split('.')[0]), data)\n",
    "    \n",
    "    ocr_embedding_list = list()\n",
    "    \n",
    "    for text in ocr_list:\n",
    "        text_token = tokenizer(text[:512], return_tensors='pt', padding=True)\n",
    "        \n",
    "        for token_item in text_token:\n",
    "            try:\n",
    "                text_token[token_item] = text_token[token_item].to(device)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        text_embedding = txt_embedding(**text_token)['pooler_output']\n",
    "        ocr_embedding_list.append(text_embedding.detach().cpu().numpy())\n",
    "    \n",
    "    data = np.concatenate(ocr_embedding_list, axis=0)\n",
    "    np.save(os.path.join('./%s/ocr_feature' % data_type, item.split('.')[0]), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resplit_text(text_list, target_length):\n",
    "    while True:\n",
    "        new_text_list = list()\n",
    "        idx = 0\n",
    "\n",
    "        while True:\n",
    "            if (idx + 1) < len(text_list):\n",
    "                new_text_list.append(text_list[idx] + text_list[idx+1])\n",
    "            else:\n",
    "                new_text_list.append(text_list[idx])\n",
    "\n",
    "            idx += 2\n",
    "\n",
    "            if idx >= len(text_list):\n",
    "                break\n",
    "\n",
    "        if len(new_text_list) <= target_length:\n",
    "            break\n",
    "        else:\n",
    "            text_list = new_text_list\n",
    "    \n",
    "    return new_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n"
     ]
    }
   ],
   "source": [
    "# extract text feature\n",
    "\n",
    "for idx, item in enumerate(os.listdir(text_dir)):\n",
    "    if idx % 100 == 0:\n",
    "        print(idx)\n",
    "        \n",
    "    item_name = item.split('.')[0]\n",
    "    \n",
    "    if os.path.exists(os.path.join('./%s/text_feature_wwm' % data_type, item_name + '.npy')):\n",
    "        continue\n",
    "    \n",
    "    item_path = os.path.join(text_dir, item)\n",
    "    \n",
    "    with open(item_path, 'r') as file:\n",
    "        text_content = json.load(file)\n",
    "        \n",
    "    asr_list = text_content[\"video_asr\"].split('|')\n",
    "    ocr_list = text_content[\"video_ocr\"].split(\"|\")\n",
    "    \n",
    "    # 根据list长度 对文字进行重新切分和组合\n",
    "    # 使得长度不超过35\n",
    "    \n",
    "    if len(asr_list) > 35:\n",
    "        asr_list = resplit_text(asr_list, 35)\n",
    "        \n",
    "    if len(ocr_list) > 35:\n",
    "        ocr_list = resplit_text(ocr_list, 35)\n",
    "    \n",
    "    text_list = asr_list + ocr_list\n",
    "    \n",
    "    embedding_list = list()\n",
    "    \n",
    "    for text in text_list:\n",
    "        text_token = tokenizer(text[:512], return_tensors='pt', padding=True)\n",
    "        \n",
    "        for token_item in text_token:\n",
    "            try:\n",
    "                text_token[token_item] = text_token[token_item].to(device)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        text_embedding = txt_embedding(**text_token)['pooler_output']\n",
    "        embedding_list.append(text_embedding.detach().cpu().numpy())\n",
    "    \n",
    "    data = np.concatenate(embedding_list, axis=0)\n",
    "    np.save(os.path.join('./%s/text_feature_wwm' % data_type, item.split('.')[0]), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-1.8",
   "language": "python",
   "name": "torch-1.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
